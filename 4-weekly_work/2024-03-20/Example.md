# Uncrtains--去云模型复现

## 模型
![image](https://github.com/ZYJ-Group/Tanghy/assets/94824386/588a9d1a-8ec9-43cb-a263-e32dc5882a7d)  
UnCRtainTS。该网络由三个主要部分组成，沿着 MBConv 块的主分支应用，该块以全输入分辨率处理特征图：首先，编码器并行应用于 T 时间点。然后，基于注意力的时间聚合器通过将 L-TAE 应用于下采样特征图来计算注意力掩模，用于聚合观察序列。最后，时间整合的特征图由解码块处理，产生图像重建和任意不确定性。

对于单时间考虑，我们使用相同的架构，并简单地丢弃不必要的基于 L-TAE 的聚合。
![image](https://github.com/ZYJ-Group/Tanghy/assets/94824386/4c5ea776-4858-4f85-bfce-17913968d064)  

## MBConv
MBConv Block是一种专为提高移动设备上的神经网络效率而设计的模块，其通过独特的结构设计来减少计算成本同时保持甚至提升性能。下面我们将详细、清晰地解析MBConv Block的工作流程及其各层的具体情况：
1. 扩张层（Expansion Layer）
功能：该层的主要作用是扩张输入特征图的通道数（即深度），以便在进行深度可分离卷积前能够捕获更多的特征。
实现方式：使用1x1的卷积操作来增加通道数，比如将通道数增加到原来的6倍。这一步骤也被称为点卷积（pointwise convolution）。
目的：增加模型的表达能力，为深度可分离卷积准备更丰富的特征表示。
2. 深度可分离卷积层（Depthwise Separable Convolution）
这一步实际上包含了两个子步骤：深度卷积（Depthwise Convolution）和点卷积（Pointwise Convolution）。

- 深度卷积：  
功能：对每个输入通道独立应用卷积核，这样做可以有效提取空间特征而不增加额外的参数和计算量。
实现方式：使用大小通常为3x3的卷积核，每个通道使用独立的卷积核进行滤波。
目的：以较低的计算成本高效提取特征图中的空间特征。

- 点卷积：  
功能：通过1x1的卷积操作，组合深度卷积的输出特征图中的通道，进行特征混合。
实现方式：每个1x1的卷积核跨越所有输入通道，用于合并深度卷积的输出。
目的：在不显著增加计算成本的前提下，有效地融合特征。

3. 压缩层（Projection Layer）
功能：该层通过1x1的卷积操作减少通道数，从而压缩特征图的维度，减少后续操作的计算量。
实现方式：使用1x1的卷积核，将扩张后的特征图通道数降低，通常降低到接近输入层的通道数。
目的：减少模型的计算复杂度和参数数量，提高模型的运行效率。

4. 残差连接（Residual Connection）
功能：如果输入和输出的尺寸相匹配，残差连接会将输入直接加到压缩层的输出上。
实现方式：直接将输入特征图与输出特征图相加。
目的：提高网络的训练稳定性，帮助梯度直接流向更深的网络层，解决深层网络训练时的梯度消失问题。

5. 激活函数
在扩张层和深度可分离卷积的深度卷积后通常会应用非线性激活函数（如ReLU或Swish），以增加网络的非线性表达能力。

这个结构的关键优点在于它显著减少了模型的参数数量和计算量，而不牺牲太多的性能，非常适合用于计算资源有限的设备上。通过这种方式，MBConv Block为构建高效且强大的神经网络模型提供了一个优秀的基础。
