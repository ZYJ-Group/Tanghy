# Domain Adaptation for Image Dehazing

## 摘要
近年来，使用基于学习的方法进行图像去雾已经取得了最先进的性能。然而，大多数现有方法在合成模糊图像上训练去雾模型，由于域偏移，这些模型不太能够很好地推广到真实的模糊图像。为了解决这个问题，我们提出了一种域适应范式，它由一个图像翻译模块和两个图像去雾模块组成。具体来说，我们首先应用双向翻译网络，通过将图像从一个域翻译到另一个域来弥合合成域和真实域之间的差距。然后，我们使用翻译前后的图像来训练所提出的两个具有一致性约束的图像去雾网络。在这个阶段，我们通过利用清晰图像的特性（例如暗通道先验和图像梯度平滑）将真实的有雾图像纳入去雾训练中，以进一步提高域自适应性。通过端到端的方式训练图像平移和去雾网络，我们可以获得更好的图像平移和去雾效果。合成图像和真实图像的实验结果表明，我们的模型比最先进的去雾算法表现得更好。

## 简介
我们希望真实有雾图像的去雾结果能够具有清晰图像的一些特性，例如暗通道先验和图像梯度平滑。我们以端到端的方式训练图像翻译网络和去雾网络，以便它们可以相互改进。如图 1 所示，与 EPDN 最近的去雾工作相比，我们的模型产生了更清晰的图像 [25]。我们将我们工作的贡献总结如下： 

• 我们提出了一种用于图像去雾的端到端域适应框架，它有效地弥合了合成图像和真实世界模糊图像之间的差距。 • 我们证明，将真实的模糊图像纳入训练过程可以提高去雾性能。 

• 我们对合成数据集和现实世界的模糊图像进行了广泛的实验，这表明所提出的方法优于最先进的去雾方法。

## 相关工作
然而，由于合成数据和真实数据之间的域差距，在合成图像上训练的基于 CNN 的模型在应用于真实域时往往会出现显着的性能下降。为此，李等人。 [14]提出了一种半监督去雾模型，该模型在合成和真实雾霾图像上进行训练，因此具有合成和真实雾霾图像之间的域自适应性。然而，仅仅应用真实的雾霾图像进行训练并不能真正解决域转移的问题。与上述方法不同，我们的模型首先应用图像翻译网络将图像从一个域翻译到另一个域，然后使用翻译后的图像及其原始图像（合成或真实）在合成域和真实域上执行图像去雾。提议的方法可以有效解决域转移问题。

### 领域适应
领域适应旨在减少不同领域之间的差异[1,6,20]。现有的工作要么执行特征级要么像素级适应。特征级适应方法旨在通过最小化最大平均差异[19]或在特征空间上应用对抗性学习策略[32, 31]来对齐源域和目标域之间的特征分布。另一条研究重点是像素级自适应[3,28,7]。这些方法通过应用图像到图像翻译 [3, 28] 学习或风格迁移 [7] 方法来增加目标域中的数据来处理域转移问题。

最近，许多方法在许多视觉任务中联合执行特征级和像素级自适应，例如图像分类[10]、语义分割[5]和深度预测[37]。这些方法 [5, 37] 通过图像到图像转换网络（例如 CycleGAN [38]），通过像素级自适应将图像从一个域转换到另一个域。然后将翻译后的图像输入到具有特征级对齐的任务网络中。

在这项工作中，我们利用 CycleGAN 将真实的模糊图像适应我们在合成数据上训练的去雾模型。此外，由于深度信息与图像雾度的形成密切相关，因此我们将深度信息合并到翻译网络中，以更好地指导真实的雾度图像翻译。

## 提出的方法
### 方法概述
给定合成数据集 XS = {xs, ys}Nl s=1 和真实有雾图像集 XR = {xr}Nur=1，其中 Nl 和 Nu 分别表示合成和真实有雾图像的数量。我们的目标是学习一个单一的图像去雾模型，它可以准确地从真实的有雾图像中预测出清晰的图像。由于域转移，仅在合成数据上训练的去雾模型不能很好地推广到真实的有雾图像。为了解决这个问题，我们提出了一个域适应框架，它由两个主要部分组成：图像翻译网络GS→R和GR→S，以及两个去雾网络GS和GR。图像翻译网络将图像从一个域翻译到另一个域，以弥合它们之间的差距。然后，去雾网络使用翻译图像和源图像（例如，合成图像或真实图像）执行图像去雾。如图 2 所示，所提出的模型采用真实模糊图像 xr 和合成图像 xs 及其相应的深度图像 ds 作为输入。我们首先得到对应的平移图像 xs→r = GS→R(xs, ds)和 xr→s = GR→S(xr) 使用两个图像转换器。然后，我们将xs和xr→s传递给GS，xr和xr→s传递给GR来执行图像去雾。

### 图像翻译模块
图像翻译模块包括两个翻译器：合成到真实网络GS→R和真实到合成网络GR→S。 GS→R网络以(Xs,Ds)作为输入，生成与真实有雾图像风格相似的平移图像GS→R(Xs,Ds)。另一个翻译器GR→S执行相反的图像翻译。由于深度信息与雾化公式高度相关，因此我们将其合并到生成器 GS→R 中，以在实际情况下生成具有相似雾度分布的图像。我们采用空间特征变换（SFT）层[33, 15]将深度信息合并到翻译网络中，可以有效地融合深度图和合成图像的特征。如图3所示，SFT层首先应用三个卷积层从深度图中提取条件图φ。然后将条件图输入到其他两个卷积层以分别预测调制参数 γ 和 β。最后，我们可以通过以下方式获得输出移位​​特征：SFT(F |γ, β) = γ ⊙ F + β。 (2) 其中 ⊙ 是逐元素乘法。在翻译器GS→R中，我们以深度图为指导，使用SFT层来变换倒数第二个卷积层的特征。如图4所示，平移后的合成图像相对更接近真实世界的模糊图像。我们在表 1 中展示了翻译器 GS→R 的详细配置。我们还采用了 CycleGAN [38] 提供的架构，用于生成器 GR→S 和鉴别器（Dimg R 和 Dimg S ）。

### 除雾模块
我们的方法包括两个去雾模块 GS 和 GR，分别对合成域和真实域执行图像去雾。 GS将合成图像xs和平移图像xr→s作为输入来执行图像去雾。 GR 在 xr 和 xs→r 上进行训练。对于这两个图像去雾网络，我们都使用具有跳过连接和侧输出的标准编码器解码器架构如[37]。每个域中的去雾网络共享相同的网络架构，但具有不同的学习参数。

### 训练损失
在域适应框架中，我们采用以下损失来训练网络。图像翻译损失。我们翻译模块的目的是学习翻译器 GS→R 和 GR→S，以减少合成域 XS 和真实域 XR 之间的差异。对于翻译器 GS→R，我们期望 GS→R(xs, ds) 与真实的有雾图像 xr 无法区分。因此，我们采用图像级鉴别器 Dimg R 和特征级鉴别器 Dfeat R ，通过对抗性学习方式执行最小最大游戏。 Dimg R 旨在对齐真实图像 xr 和平移图像 GS→R(xs, ds) 之间的分布。判别器 Dfeat R 有助于对齐 xr 和 GS→R(xs, ds) 的特征图之间的分布。对抗性损失定义为： Limg Gan(XR, (XS , DS ), Dimg R , GS→R) = Exs∼XS ,ds∼DS [Dimg R (GS→R(xs, ds))] +Exr∼ XR [Dimg R (xr) − 1]。 (3) Lf eat Gan(XR, (XS , DS ), Dfeat R , GS→R, GR) = Exs∼XS ,ds∼DS [Df eat R (GR(GS→R(xs, ds)))] +Exr∼XR [Df 吃 R (GR(xr)) − 1]。 (4) 与 GS→R 类似，翻译器 GR→S 还有另一个图像级对抗性损失和特征级对抗性损失，记为 Limg Gan(XS , XR, Dimg S , GR→S )，Lf eat Gan （XS，XR，Dfeat S，GR→S，GS），分别。此外，我们利用循环一致性损失[38]来规范翻译网络的训练。具体来说，当将图像 xs 依次传递给 GS→R 和 GR→S 时，我们期望输出应该是相同的图像，对于 xr 反之亦然。即，GR→S(GS→R(xs, ds)) ≈ xs 且 GS→R(GR→S(xr), dr) ≈ xr。循环一致性损失可以表示为： Lc = Exs∼XS,ds∼DS [||GR→S (GS→R(xs, ds) − xs||1] + Exr∼XR,dr∼DR [|| GS→R(GR→S (xr), dr) − xr||1]。 (5) 最后，为了鼓励生成器保留输入和输出之间的内容信息，我们还利用了恒等映射损失[38]，表示为： Lidt = Exs∼XS [||GR→S (xs) − xs||1] +Exr∼XR,dr∼DR [||GS→R(xr, dr) − xr||1] (6) 翻译模块的完整损失函数如下： Ltran = Limg Gan(XR, (XS , DS ), Dimg R , GS→R) +Lf eat Gan(XR, (XS , DS ), Dfeat R , GS→R, GR) +Limg Gan(XS , XR, Dimg S , GR→S ) +Lf eat Gan (XS , XR, Dfeat S , GR→S , GS ) +λ1Lc + λ2Lidt。

### 图像去雾损失
现在，我们可以将合成图像 XS 和相应的深度图像 DS 传输到生成器 GS→R，并获得新的数据集 XS→R = GS→R(XS, DS)，其与真实模糊图像具有相似的风格。然后，我们以半监督的方式在 XS→R 和 XR 上训练图像去雾网络 GR。对于监督分支，我们应用均方损失来确保预测图像 JS→R 接近干净图像 YS，可以定义为： Lrm = ‖JS→R − YS ‖2 2 。 （8）在无监督分支中，我们引入了总变分和暗通道损失，它们对去雾网络进行正则化，以产生具有与清晰图像相似的统计特征的图像。总变异损失是预测图像 JR 上先验的 l1 正则化梯度：Lrt = ‖∂hJR‖1 + ‖∂vJR‖​​1。其中 ∂h 表示水平梯度算子，∂v 表示垂直梯度算子。此外，最近的工作[9]提出了暗通道的概念，可以表示为： D(I) = min y∈N (x) [ min c∈{r,g,b} I c(y )]。 (10) 其中x和y是图像I的像素坐标，Ic表示I的第c个颜色通道，N(x)表示以x为中心的局部邻域。他等人。 [9]还表明，暗通道图像的大多数强度为零或接近于零。因此，我们应用以下暗通道（DC）损失来确保预测图像的暗通道与干净图像的暗通道一致：Lrd = ‖D(JR)‖1。 (11) 此外，我们还在 XS 和 XR→S 上训练互补图像去雾网络 GS。类似地，我们应用相同的监督损失和无监督损失来训练去雾网络 GS，如下： Lsm = ‖JS − YS ‖2 2 , (12) Lst = ‖∂hJR→S ‖1 + ‖∂vJR→ S ‖1, (13) Lsd = ‖D(JR→S)‖1。 (14) 最后，考虑到两个去雾网络的输出对于真实有雾图像应该是一致的，即 GR(XR) ≈ GS(GR→S(XR))，我们引入以下一致性损失： Lc = ‖GR( XR) − GS(GR→S(XR))‖1。 (15) 总体损失函数。整体损失函数定义如下：L = Ltran + λm(Lrm + Lsm) + λd(Lrd + Lsd) +λt(Lrt + Lst) + λcLc。 (16) 其中 λm、λd、λt 和 λc 是权衡权重。

## 实验结果
### 实验细节
数据集。我们从 RESIDE 数据集 [13] 中随机选择合成图像和真实模糊图像进行训练。数据集分为五个子集，即ITS（室内训练集）、OTS（室外训练集）、SOTS（合成对象测试集）、URHI（未注释的真实模糊图像）和RTTS（真实任务驱动测试集） 。对于合成数据集，我们选择 6000 个合成模糊图像进行训练，其中 3000 个来自 ITS，3000 个来自 OTS。对于真实世界的模糊图像，我们通过从 URHI 中随机选择 1000 个真实模糊图像来训练网络。在训练阶段，我们随机将所有图像裁剪为 256 × 256，并将像素值归一化为 [−1, 1]。培训细节。我们在 PyTorch [24] 中实现我们的框架，并利用批量大小为 2 的 ADAM [11] 优化器来训练网络。首先，我们将图像翻译网络 GS→R 和 GR→S 训练 90 个 epoch，动量 β1 = 0.5，β2 = 0.999，学习率设置为 5 × 10−5。然后，我们使用预训练的 GS→R 和 GR→S 模型在 {XR, GS→R(Xs, Ds)} 上训练 GR，在 {XS, GR→S(XR)} 上训练 GS 90 个 epoch。动量和学习率设置为：β1 = 0.95，β2 = 0.999，lr = 10−4。最后，我们使用上述预训练模型对整个网络进行微调。在计算 DC 损耗时，我们将 patch 设置为 35 × 35。权衡权重设置为：λtran = 1、λm = 10、λd = 10−2、λt = 10−3 和 λc = 10−1。

